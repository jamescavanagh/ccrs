{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Data for this app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install stopwordsiso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "# NLP\n",
    "import jieba\n",
    "import stopwordsiso #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('/home/jentlejames/Projects/Data/Chinese Automation/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jentlejames/Projects/Data/Chinese Automation/data')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../db/ccrs.db')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hanzi = pd.read_csv(DATA_DIR/'extracted'/'uniqueCharacters.csv',index_col=0)\n",
    "df_hanzi['hanzi_index'] = df_hanzi.index + 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hanzi</th>\n",
       "      <th>raw_frequency</th>\n",
       "      <th>pinyin</th>\n",
       "      <th>definition</th>\n",
       "      <th>stroke_count</th>\n",
       "      <th>hanzi_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>昏</td>\n",
       "      <td>94.970535</td>\n",
       "      <td>hūn</td>\n",
       "      <td>muddle-headed/twilight/to faint/to lose consci...</td>\n",
       "      <td>8</td>\n",
       "      <td>1001560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7916</th>\n",
       "      <td>禜</td>\n",
       "      <td>99.998307</td>\n",
       "      <td>yǒng</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>1007916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4652</th>\n",
       "      <td>痿</td>\n",
       "      <td>99.879532</td>\n",
       "      <td>wěi</td>\n",
       "      <td>atrophy</td>\n",
       "      <td>13</td>\n",
       "      <td>1004652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     hanzi  raw_frequency pinyin  \\\n",
       "1560     昏      94.970535    hūn   \n",
       "7916     禜      99.998307   yǒng   \n",
       "4652     痿      99.879532    wěi   \n",
       "\n",
       "                                             definition stroke_count  \\\n",
       "1560  muddle-headed/twilight/to faint/to lose consci...            8   \n",
       "7916                                                NaN           15   \n",
       "4652                                            atrophy           13   \n",
       "\n",
       "      hanzi_index  \n",
       "1560      1001560  \n",
       "7916      1007916  \n",
       "4652      1004652  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter by column\n",
    "hanziColumns = ['char','cumulativeRawFrequency','kMandarin','English','kTotalStrokes','hanzi_index']\n",
    "df_hanzi = df_hanzi[hanziColumns].copy()\n",
    "\n",
    "# Rename for sql column standard\n",
    "df_hanzi.columns = ['hanzi','raw_frequency','pinyin','definition','stroke_count','hanzi_index']\n",
    "df_hanzi.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9933 entries, 0 to 9932\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   hanzi          9933 non-null   object \n",
      " 1   raw_frequency  9933 non-null   float64\n",
      " 2   pinyin         9931 non-null   object \n",
      " 3   definition     6224 non-null   object \n",
      " 4   stroke_count   9933 non-null   object \n",
      " 5   hanzi_index    9933 non-null   int64  \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 543.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_hanzi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hanzi                        深\n",
       "raw_frequency        71.059093\n",
       "pinyin                    shēn\n",
       "definition       deep/profound\n",
       "stroke_count                11\n",
       "hanzi_index            1000400\n",
       "Name: 400, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hanzi.iloc[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        8\n",
       "1        1\n",
       "2        9\n",
       "3        4\n",
       "4        2\n",
       "        ..\n",
       "9928    20\n",
       "9929    22\n",
       "9930    23\n",
       "9931    14\n",
       "9932    16\n",
       "Name: stroke_count, Length: 9933, dtype: int16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaning up some ambivalence to convert to int type\n",
    "df_hanzi[df_hanzi['stroke_count'].str.contains(' ')] = 9\n",
    "\n",
    "\n",
    "df_hanzi.stroke_count.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'DROP TABLE \"hanzi_info\"': database is locked",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/sql.py:2018\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2017\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2018\u001b[0m     cur\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2019\u001b[0m     \u001b[39mreturn\u001b[39;00m cur\n",
      "\u001b[0;31mOperationalError\u001b[0m: database is locked",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_hanzi\u001b[39m.\u001b[39;49mto_sql(\u001b[39m'\u001b[39;49m\u001b[39mhanzi_info\u001b[39;49m\u001b[39m'\u001b[39;49m,conn,if_exists\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mreplace\u001b[39;49m\u001b[39m'\u001b[39;49m,index\u001b[39m=\u001b[39;49mdf_hanzi[\u001b[39m'\u001b[39;49m\u001b[39mhanzi_index\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py:2987\u001b[0m, in \u001b[0;36mNDFrame.to_sql\u001b[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2831\u001b[0m \u001b[39mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[1;32m   2832\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2983\u001b[0m \u001b[39m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[1;32m   2984\u001b[0m \u001b[39m\"\"\"\u001b[39;00m  \u001b[39m# noqa:E501\u001b[39;00m\n\u001b[1;32m   2985\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m sql\n\u001b[0;32m-> 2987\u001b[0m \u001b[39mreturn\u001b[39;00m sql\u001b[39m.\u001b[39;49mto_sql(\n\u001b[1;32m   2988\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2989\u001b[0m     name,\n\u001b[1;32m   2990\u001b[0m     con,\n\u001b[1;32m   2991\u001b[0m     schema\u001b[39m=\u001b[39;49mschema,\n\u001b[1;32m   2992\u001b[0m     if_exists\u001b[39m=\u001b[39;49mif_exists,\n\u001b[1;32m   2993\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   2994\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[1;32m   2995\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m   2996\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   2997\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m   2998\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/sql.py:695\u001b[0m, in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(frame, DataFrame):\n\u001b[1;32m    691\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    692\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m'\u001b[39m\u001b[39m argument should be either a Series or a DataFrame\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    693\u001b[0m     )\n\u001b[0;32m--> 695\u001b[0m \u001b[39mreturn\u001b[39;00m pandas_sql\u001b[39m.\u001b[39;49mto_sql(\n\u001b[1;32m    696\u001b[0m     frame,\n\u001b[1;32m    697\u001b[0m     name,\n\u001b[1;32m    698\u001b[0m     if_exists\u001b[39m=\u001b[39;49mif_exists,\n\u001b[1;32m    699\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    700\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[1;32m    701\u001b[0m     schema\u001b[39m=\u001b[39;49mschema,\n\u001b[1;32m    702\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m    703\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    704\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    705\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[1;32m    706\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mengine_kwargs,\n\u001b[1;32m    707\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/sql.py:2187\u001b[0m, in \u001b[0;36mSQLiteDatabase.to_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, **kwargs)\u001b[0m\n\u001b[1;32m   2176\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcol\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mmy_type\u001b[39m}\u001b[39;00m\u001b[39m) not a string\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2178\u001b[0m table \u001b[39m=\u001b[39m SQLiteTable(\n\u001b[1;32m   2179\u001b[0m     name,\n\u001b[1;32m   2180\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2185\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   2186\u001b[0m )\n\u001b[0;32m-> 2187\u001b[0m table\u001b[39m.\u001b[39;49mcreate()\n\u001b[1;32m   2188\u001b[0m \u001b[39mreturn\u001b[39;00m table\u001b[39m.\u001b[39minsert(chunksize, method)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/sql.py:831\u001b[0m, in \u001b[0;36mSQLTable.create\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTable \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m already exists.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    830\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mif_exists \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mreplace\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 831\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpd_sql\u001b[39m.\u001b[39;49mdrop_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mschema)\n\u001b[1;32m    832\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute_create()\n\u001b[1;32m    833\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mif_exists \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/sql.py:2202\u001b[0m, in \u001b[0;36mSQLiteDatabase.drop_table\u001b[0;34m(self, name, schema)\u001b[0m\n\u001b[1;32m   2200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop_table\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m, schema: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2201\u001b[0m     drop_sql \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDROP TABLE \u001b[39m\u001b[39m{\u001b[39;00m_get_valid_sqlite_name(name)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 2202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(drop_sql)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/sql.py:2030\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2027\u001b[0m     \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39minner_exc\u001b[39;00m\n\u001b[1;32m   2029\u001b[0m ex \u001b[39m=\u001b[39m DatabaseError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecution failed on sql \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00margs[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2030\u001b[0m \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'DROP TABLE \"hanzi_info\"': database is locked"
     ]
    }
   ],
   "source": [
    "df_hanzi.to_sql('hanzi_info',conn,if_exists='replace',index=df_hanzi['hanzi_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hanzi</th>\n",
       "      <th>raw_frequency</th>\n",
       "      <th>pinyin</th>\n",
       "      <th>definition</th>\n",
       "      <th>stroke_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hanzi_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001817</th>\n",
       "      <td>廉</td>\n",
       "      <td>96.376082</td>\n",
       "      <td>lián</td>\n",
       "      <td>incorrupt/inexpensive</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006055</th>\n",
       "      <td>織</td>\n",
       "      <td>99.980609</td>\n",
       "      <td>zhī</td>\n",
       "      <td>None</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006682</th>\n",
       "      <td>箜</td>\n",
       "      <td>99.992575</td>\n",
       "      <td>kōng</td>\n",
       "      <td>ancient string music instrument</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            hanzi  raw_frequency pinyin                       definition  \\\n",
       "hanzi_index                                                                \n",
       "1001817         廉      96.376082   lián            incorrupt/inexpensive   \n",
       "1006055         織      99.980609    zhī                             None   \n",
       "1006682         箜      99.992575   kōng  ancient string music instrument   \n",
       "\n",
       "            stroke_count  \n",
       "hanzi_index               \n",
       "1001817               13  \n",
       "1006055               18  \n",
       "1006682               14  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hanzi = pd.read_sql('SELECT * FROM hanzi_info', conn, index_col='hanzi_index')\n",
    "test_hanzi.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9933 entries, 0 to 9932\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   hanzi          9933 non-null   object \n",
      " 1   raw_frequency  9933 non-null   float64\n",
      " 2   pinyin         9931 non-null   object \n",
      " 3   definition     6224 non-null   object \n",
      " 4   stroke_count   9933 non-null   object \n",
      " 5   hanzi_index    9933 non-null   int64  \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 543.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_hanzi.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_radicals = pd.read_csv(DATA_DIR/'extracted'/'Radicals.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective 2: Meaning and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one</td>\n",
       "      <td>a, an</td>\n",
       "      <td>alone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>number one</td>\n",
       "      <td>line</td>\n",
       "      <td>Kangxi radical 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>line</td>\n",
       "      <td>Kangxi radical 4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>second</td>\n",
       "      <td>2nd heavenly stem</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hook</td>\n",
       "      <td>Kangxi radical 6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0                   1                  2    3    4\n",
       "0         one               a, an              alone  NaN  NaN\n",
       "1  number one                line   Kangxi radical 2  NaN  NaN\n",
       "2        line    Kangxi radical 4                NaN  NaN  NaN\n",
       "3      second   2nd heavenly stem                NaN  NaN  NaN\n",
       "4        hook    Kangxi radical 6                NaN  NaN  NaN"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# English Category Clean up \n",
    "# Remove whitespace formatting\n",
    "df_radicals['english']  = df_radicals.english.str.replace('\\xad','')\n",
    "\n",
    "# Cleaning up the list of definitions \n",
    "\n",
    "# Needs to deal with the nested list, expanding it out into a table\n",
    "df_radicals['Meaning'] = df_radicals['kDefinition'].str.split(';')\n",
    "df_meaning = df_radicals['Meaning'].apply(pd.Series).copy()\n",
    "df_meaning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the extra information about Kangxi Radicals\n",
    "for i in range(5):\n",
    "    df_meaning[i] = np.where(df_meaning[i].str.contains('Kangxi'),np.NaN,df_meaning[i])\n",
    "    df_meaning[i] = df_meaning[i].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>radical</th>\n",
       "      <th>variants</th>\n",
       "      <th>simplifiedradical</th>\n",
       "      <th>pinyin</th>\n",
       "      <th>english</th>\n",
       "      <th>strokecount</th>\n",
       "      <th>char</th>\n",
       "      <th>ucn</th>\n",
       "      <th>kDefinition</th>\n",
       "      <th>Meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>一</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yi1</td>\n",
       "      <td>one</td>\n",
       "      <td>1</td>\n",
       "      <td>一</td>\n",
       "      <td>U+4E00</td>\n",
       "      <td>one; a, an; alone</td>\n",
       "      <td>[one,  a, an,  alone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>丨</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gun3</td>\n",
       "      <td>line</td>\n",
       "      <td>1</td>\n",
       "      <td>丨</td>\n",
       "      <td>U+4E28</td>\n",
       "      <td>number one; line; Kangxi radical 2</td>\n",
       "      <td>[number one,  line,  Kangxi radical 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>丿</td>\n",
       "      <td>乀 (fu2), 乁(yi2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pie3</td>\n",
       "      <td>slash</td>\n",
       "      <td>1</td>\n",
       "      <td>丿</td>\n",
       "      <td>U+4E3F</td>\n",
       "      <td>line; Kangxi radical 4</td>\n",
       "      <td>[line,  Kangxi radical 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>乙</td>\n",
       "      <td>乚 (yin3), 乛</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yi4</td>\n",
       "      <td>second</td>\n",
       "      <td>1</td>\n",
       "      <td>乙</td>\n",
       "      <td>U+4E59</td>\n",
       "      <td>second; 2nd heavenly stem</td>\n",
       "      <td>[second,  2nd heavenly stem]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>亅</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jue2</td>\n",
       "      <td>hook</td>\n",
       "      <td>1</td>\n",
       "      <td>亅</td>\n",
       "      <td>U+4E85</td>\n",
       "      <td>hook; Kangxi radical 6</td>\n",
       "      <td>[hook,  Kangxi radical 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number radical         variants simplifiedradical pinyin english  \\\n",
       "0       1       一              NaN               NaN    yi1     one   \n",
       "1       2       丨              NaN               NaN   gun3    line   \n",
       "2       4       丿  乀 (fu2), 乁(yi2)               NaN   pie3   slash   \n",
       "3       5       乙      乚 (yin3), 乛               NaN    yi4  second   \n",
       "4       6       亅              NaN               NaN   jue2    hook   \n",
       "\n",
       "   strokecount char     ucn                         kDefinition  \\\n",
       "0            1    一  U+4E00                   one; a, an; alone   \n",
       "1            1    丨  U+4E28  number one; line; Kangxi radical 2   \n",
       "2            1    丿  U+4E3F              line; Kangxi radical 4   \n",
       "3            1    乙  U+4E59           second; 2nd heavenly stem   \n",
       "4            1    亅  U+4E85              hook; Kangxi radical 6   \n",
       "\n",
       "                                  Meaning  \n",
       "0                   [one,  a, an,  alone]  \n",
       "1  [number one,  line,  Kangxi radical 2]  \n",
       "2               [line,  Kangxi radical 4]  \n",
       "3            [second,  2nd heavenly stem]  \n",
       "4               [hook,  Kangxi radical 6]  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_radicals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line is to unpack the definitions even further, with the goal of\n",
    "# unpacking the nested lists inside of the nested lists\n",
    "\n",
    "# Populating an empty array\n",
    "df_meaning['idx'] = np.NaN\n",
    "\n",
    "# Recurses through each column, adding where it iis found  \n",
    "for i in range(5):\n",
    "    df_meaning['idx'] = np.where(df_radicals['english'] == df_meaning[i],i,df_meaning['idx'])\n",
    "\n",
    "\n",
    "# Checking for redundant definitions\n",
    "secondaryCheckIdx = df_meaning['idx'].isnull()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meaning['english'] = df_radicals['english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpacking Level 2 nested list of definitions, checking for matches\n",
    "#df_meaning[df_meaning[4].str.contains(',') == True]\n",
    "\n",
    "\n",
    "commaMeanings0 = df_meaning[secondaryCheckIdx][0].str.split(', | or ').apply(pd.Series)\n",
    "#print(commaMeanings0.shape[1])\n",
    "commaMeanings1 = df_meaning[secondaryCheckIdx][1].str.split(', | or ').apply(pd.Series)\n",
    "#print(commaMeanings1.shape[0])\n",
    "# Merging two nested lists together in order to check for matching words that indicate redudant information \n",
    "commaMeanings = pd.merge(commaMeanings0,commaMeanings1,how='outer',on=commaMeanings0.index).drop('key_0',axis=1)\n",
    "\n",
    "# Makes possible to iterate through each\n",
    "commaMeanings.columns = range(commaMeanings.shape[1])\n",
    "\n",
    "commaMeanings['single_word_def_is_redundant'] = np.NaN\n",
    "commaMeanings['english'] = df_meaning[secondaryCheckIdx].english.reset_index(drop=True)\n",
    "\n",
    "for i in range(commaMeanings.shape[1] -2 ): # -2 for index column and english column\n",
    "    commaMeanings['single_word_def_is_redundant'] = np.where(commaMeanings['english'] == commaMeanings[i], i, commaMeanings['single_word_def_is_redundant'])\n",
    "\n",
    "commaMeanings['merge_idx'] =  df_meaning[secondaryCheckIdx].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214, 8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meaning = pd.merge(df_meaning,commaMeanings[['merge_idx','single_word_def_is_redundant']],how='left',left_on=df_meaning.index,right_on='merge_idx').drop('merge_idx',axis=1)\n",
    "df_meaning.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meaning['english'] = np.where(df_meaning['single_word_def_is_redundant'].isnull() & df_meaning['idx'].isnull(),df_meaning['english'],np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meaning = df_meaning[['english',0,1,2,3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>one</td>\n",
       "      <td>a, an</td>\n",
       "      <td>alone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>number one</td>\n",
       "      <td>line</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slash</td>\n",
       "      <td>line</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>second</td>\n",
       "      <td>2nd heavenly stem</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>hook</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>NaN</td>\n",
       "      <td>even, uniform, of equal length</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>tooth</td>\n",
       "      <td>teeth</td>\n",
       "      <td>gears, cogs</td>\n",
       "      <td>age</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>NaN</td>\n",
       "      <td>dragon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>NaN</td>\n",
       "      <td>turtle or tortoise</td>\n",
       "      <td>cuckold</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>NaN</td>\n",
       "      <td>flute</td>\n",
       "      <td>pipe, ancient measure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    english                               0                      1      2  \\\n",
       "0       NaN                             one                  a, an  alone   \n",
       "1       NaN                      number one                   line    NaN   \n",
       "2     slash                            line                    NaN    NaN   \n",
       "3       NaN                          second      2nd heavenly stem    NaN   \n",
       "4       NaN                            hook                    NaN    NaN   \n",
       "..      ...                             ...                    ...    ...   \n",
       "209     NaN  even, uniform, of equal length                    NaN    NaN   \n",
       "210   tooth                           teeth            gears, cogs    age   \n",
       "211     NaN                          dragon                    NaN    NaN   \n",
       "212     NaN              turtle or tortoise                cuckold    NaN   \n",
       "213     NaN                           flute  pipe, ancient measure    NaN   \n",
       "\n",
       "       3    4  \n",
       "0    NaN  NaN  \n",
       "1    NaN  NaN  \n",
       "2    NaN  NaN  \n",
       "3    NaN  NaN  \n",
       "4    NaN  NaN  \n",
       "..   ...  ...  \n",
       "209  NaN  NaN  \n",
       "210  NaN  NaN  \n",
       "211  NaN  NaN  \n",
       "212  NaN  NaN  \n",
       "213  NaN  NaN  \n",
       "\n",
       "[214 rows x 6 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_radicals['Meaning'] = df_meaning.apply(lambda x: ', '.join(x.dropna()), axis=1)\n",
    "df_radicals['Meaning'] = '[' + df_radicals['Meaning'] + ']'\n",
    "df_radicals.drop(['kDefinition','english'],axis=1,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radical_variants =  df_radicals['variants']#.str.split(',').dropna()\n",
    "radical_variants =  radical_variants.str.replace('\\([a-z1-4]*\\)','',regex=True).dropna()\n",
    "radical_variants.str.replace('\\s?,\\s?', ',',regex=True)\n",
    "radical_variants_unique =  radical_variants.str.split(',').apply(pd.Series).copy()\n",
    "\n",
    "radical_variants_unique = pd.concat([radical_variants_unique[0],radical_variants_unique[1]],axis=0).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merging traditional and simplified radicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>radical</th>\n",
       "      <th>variants</th>\n",
       "      <th>simplifiedradical</th>\n",
       "      <th>pinyin</th>\n",
       "      <th>strokecount</th>\n",
       "      <th>char</th>\n",
       "      <th>ucn</th>\n",
       "      <th>Meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>一</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yi1</td>\n",
       "      <td>1</td>\n",
       "      <td>一</td>\n",
       "      <td>U+4E00</td>\n",
       "      <td>[one, a, an, alone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>丨</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gun3</td>\n",
       "      <td>1</td>\n",
       "      <td>丨</td>\n",
       "      <td>U+4E28</td>\n",
       "      <td>[number one, line]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>丿</td>\n",
       "      <td>乀 (fu2), 乁(yi2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pie3</td>\n",
       "      <td>1</td>\n",
       "      <td>丿</td>\n",
       "      <td>U+4E3F</td>\n",
       "      <td>[slash, line]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>乙</td>\n",
       "      <td>乚 (yin3), 乛</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yi4</td>\n",
       "      <td>1</td>\n",
       "      <td>乙</td>\n",
       "      <td>U+4E59</td>\n",
       "      <td>[second, 2nd heavenly stem]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>亅</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jue2</td>\n",
       "      <td>1</td>\n",
       "      <td>亅</td>\n",
       "      <td>U+4E85</td>\n",
       "      <td>[hook]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>210</td>\n",
       "      <td>齊</td>\n",
       "      <td>NaN</td>\n",
       "      <td>齐</td>\n",
       "      <td>qi2</td>\n",
       "      <td>14</td>\n",
       "      <td>齊</td>\n",
       "      <td>U+9F4A</td>\n",
       "      <td>[even, uniform, of equal length]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>211</td>\n",
       "      <td>齒</td>\n",
       "      <td>NaN</td>\n",
       "      <td>齿</td>\n",
       "      <td>chi3</td>\n",
       "      <td>15</td>\n",
       "      <td>齒</td>\n",
       "      <td>U+9F52</td>\n",
       "      <td>[tooth, teeth, gears, cogs, age]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>212</td>\n",
       "      <td>龍</td>\n",
       "      <td>NaN</td>\n",
       "      <td>龙</td>\n",
       "      <td>long2</td>\n",
       "      <td>16</td>\n",
       "      <td>龍</td>\n",
       "      <td>U+9F8D</td>\n",
       "      <td>[dragon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>213</td>\n",
       "      <td>龜</td>\n",
       "      <td>NaN</td>\n",
       "      <td>龟</td>\n",
       "      <td>gui1</td>\n",
       "      <td>16</td>\n",
       "      <td>龜</td>\n",
       "      <td>U+9F9C</td>\n",
       "      <td>[turtle or tortoise, cuckold]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>214</td>\n",
       "      <td>龠</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yue4</td>\n",
       "      <td>17</td>\n",
       "      <td>龠</td>\n",
       "      <td>U+9FA0</td>\n",
       "      <td>[flute, pipe, ancient measure]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     number radical         variants simplifiedradical pinyin  strokecount  \\\n",
       "0         1       一              NaN               NaN    yi1            1   \n",
       "1         2       丨              NaN               NaN   gun3            1   \n",
       "2         4       丿  乀 (fu2), 乁(yi2)               NaN   pie3            1   \n",
       "3         5       乙      乚 (yin3), 乛               NaN    yi4            1   \n",
       "4         6       亅              NaN               NaN   jue2            1   \n",
       "..      ...     ...              ...               ...    ...          ...   \n",
       "209     210       齊              NaN                 齐    qi2           14   \n",
       "210     211       齒              NaN                 齿   chi3           15   \n",
       "211     212       龍              NaN                 龙  long2           16   \n",
       "212     213       龜              NaN                 龟   gui1           16   \n",
       "213     214       龠              NaN               NaN   yue4           17   \n",
       "\n",
       "    char     ucn                           Meaning  \n",
       "0      一  U+4E00               [one, a, an, alone]  \n",
       "1      丨  U+4E28                [number one, line]  \n",
       "2      丿  U+4E3F                     [slash, line]  \n",
       "3      乙  U+4E59       [second, 2nd heavenly stem]  \n",
       "4      亅  U+4E85                            [hook]  \n",
       "..   ...     ...                               ...  \n",
       "209    齊  U+9F4A  [even, uniform, of equal length]  \n",
       "210    齒  U+9F52  [tooth, teeth, gears, cogs, age]  \n",
       "211    龍  U+9F8D                          [dragon]  \n",
       "212    龜  U+9F9C     [turtle or tortoise, cuckold]  \n",
       "213    龠  U+9FA0    [flute, pipe, ancient measure]  \n",
       "\n",
       "[214 rows x 9 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_radicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_radicals['simplifiedradical'].fillna(df_radicals['radical'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_radicals['simplifiedradical'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting instances where there is a traditional radical\n",
    "\n",
    "df_radicals['traditional'] = np.where(df_radicals['simplifiedradical'] != df_radicals['radical'],df_radicals['radical'],np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['number', 'radical', 'variants', 'simplifiedradical', 'pinyin',\n",
       "       'strokecount', 'char', 'ucn', 'Meaning', 'traditional'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_radicals.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>radical</th>\n",
       "      <th>variants</th>\n",
       "      <th>simplifiedradical</th>\n",
       "      <th>pinyin</th>\n",
       "      <th>strokecount</th>\n",
       "      <th>char</th>\n",
       "      <th>ucn</th>\n",
       "      <th>Meaning</th>\n",
       "      <th>traditional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>又</td>\n",
       "      <td>NaN</td>\n",
       "      <td>又</td>\n",
       "      <td>you4</td>\n",
       "      <td>2</td>\n",
       "      <td>又</td>\n",
       "      <td>U+53C8</td>\n",
       "      <td>[and, also, again, in addition]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>139</td>\n",
       "      <td>色</td>\n",
       "      <td>NaN</td>\n",
       "      <td>色</td>\n",
       "      <td>se4</td>\n",
       "      <td>6</td>\n",
       "      <td>色</td>\n",
       "      <td>U+8272</td>\n",
       "      <td>[color, tint, hue, shade, form, body, beauty, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>斤</td>\n",
       "      <td>NaN</td>\n",
       "      <td>斤</td>\n",
       "      <td>jin1</td>\n",
       "      <td>4</td>\n",
       "      <td>斤</td>\n",
       "      <td>U+65A4</td>\n",
       "      <td>[axe, a catty (approximately 600 g), an axe, k...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     number radical variants simplifiedradical pinyin  strokecount char  \\\n",
       "28       29       又      NaN                 又   you4            2    又   \n",
       "137     139       色      NaN                 色    se4            6    色   \n",
       "68       69       斤      NaN                 斤   jin1            4    斤   \n",
       "\n",
       "        ucn                                            Meaning traditional  \n",
       "28   U+53C8                    [and, also, again, in addition]         NaN  \n",
       "137  U+8272  [color, tint, hue, shade, form, body, beauty, ...         NaN  \n",
       "68   U+65A4  [axe, a catty (approximately 600 g), an axe, k...         NaN  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_radicals.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_radicals.drop(['simplifiedradical','char'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_radicals.columns =['radical_number', 'radical', 'variants', 'pinyin', 'stroke_count', 'ucn',\n",
    "       'meaning', 'traditional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_radicals['radical_index'] = df_radicals.index + 100_000\n",
    "df_radicals.to_sql('radicals',conn,if_exists='replace',index=df_radicals['radical_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radical_number</th>\n",
       "      <th>radical</th>\n",
       "      <th>variants</th>\n",
       "      <th>pinyin</th>\n",
       "      <th>stroke_count</th>\n",
       "      <th>ucn</th>\n",
       "      <th>meaning</th>\n",
       "      <th>traditional</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radical_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100213</th>\n",
       "      <td>214</td>\n",
       "      <td>龠</td>\n",
       "      <td>None</td>\n",
       "      <td>yue4</td>\n",
       "      <td>17</td>\n",
       "      <td>U+9FA0</td>\n",
       "      <td>[flute, pipe, ancient measure]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100034</th>\n",
       "      <td>36</td>\n",
       "      <td>夕</td>\n",
       "      <td>None</td>\n",
       "      <td>xi1</td>\n",
       "      <td>3</td>\n",
       "      <td>U+5915</td>\n",
       "      <td>[evening, night, dusk, slanted]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100084</th>\n",
       "      <td>86</td>\n",
       "      <td>火</td>\n",
       "      <td>灬</td>\n",
       "      <td>huo3</td>\n",
       "      <td>4</td>\n",
       "      <td>U+706B</td>\n",
       "      <td>[fire, flame, burn, anger, rage]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               radical_number radical variants pinyin  stroke_count     ucn  \\\n",
       "radical_index                                                                 \n",
       "100213                    214       龠     None   yue4            17  U+9FA0   \n",
       "100034                     36       夕     None    xi1             3  U+5915   \n",
       "100084                     86       火        灬   huo3             4  U+706B   \n",
       "\n",
       "                                        meaning traditional  \n",
       "radical_index                                                \n",
       "100213           [flute, pipe, ancient measure]        None  \n",
       "100034          [evening, night, dusk, slanted]        None  \n",
       "100084         [fire, flame, burn, anger, rage]        None  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_radicals =  pd.read_sql('SELECT * FROM radicals', conn, index_col='radical_index')\n",
    "df_test_radicals.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary Words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_cedict = pd.read_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>traditional</th>\n",
       "      <th>simplified</th>\n",
       "      <th>pinyin</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8193</th>\n",
       "      <td>信神者</td>\n",
       "      <td>信神者</td>\n",
       "      <td>xin4 shen2 zhe3</td>\n",
       "      <td>a believer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98880</th>\n",
       "      <td>變為</td>\n",
       "      <td>变为</td>\n",
       "      <td>bian4 wei2</td>\n",
       "      <td>to change into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107156</th>\n",
       "      <td>金匯兌本位制</td>\n",
       "      <td>金汇兑本位制</td>\n",
       "      <td>jin1 hui4 dui4 ben3 wei4 zhi4</td>\n",
       "      <td>gold exchange standard (economics)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       traditional simplified                         pinyin  \\\n",
       "8193           信神者        信神者                xin4 shen2 zhe3   \n",
       "98880           變為         变为                     bian4 wei2   \n",
       "107156      金匯兌本位制     金汇兑本位制  jin1 hui4 dui4 ben3 wei4 zhi4   \n",
       "\n",
       "                                   english  \n",
       "8193                            a believer  \n",
       "98880                       to change into  \n",
       "107156  gold exchange standard (economics)  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cedict = pd.read_csv(DATA_DIR/'extracted'/'ce_dict.csv')\n",
    "df_cedict.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cedict['cedict_index'] = df_cedict.index + 2_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_cedict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb Cell 54\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_cedict\u001b[39m.\u001b[39mto_sql(\u001b[39m'\u001b[39m\u001b[39mce_dictionary\u001b[39m\u001b[39m'\u001b[39m,conn,if_exists\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mreplace\u001b[39m\u001b[39m'\u001b[39m,index\u001b[39m=\u001b[39mdf_cedict[\u001b[39m'\u001b[39m\u001b[39mcedict_index\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_cedict' is not defined"
     ]
    }
   ],
   "source": [
    "df_cedict.to_sql('ce_dictionary',conn,if_exists='replace',index=df_cedict['cedict_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cedict =  pd.read_sql('SELECT * FROM ce_dictionary', conn, index_col='cedict_index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['traditional', 'simplified', 'pinyin', 'english'], dtype='object')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_cedict.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HSK (used to add statistics and remove stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hsk = pd.read_csv(DATA_DIR/'HSK Standard Course 1-6-Table 1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Traditional</th>\n",
       "      <th>Simplified</th>\n",
       "      <th>English</th>\n",
       "      <th>HSK</th>\n",
       "      <th>HSK 5（二）词语搭配</th>\n",
       "      <th>Img</th>\n",
       "      <th>Txt</th>\n",
       "      <th>Pinyin</th>\n",
       "      <th>Explanation</th>\n",
       "      <th>...</th>\n",
       "      <th>Alternative</th>\n",
       "      <th>Grammar Reference</th>\n",
       "      <th>Song Lyrics</th>\n",
       "      <th>Song YouTube</th>\n",
       "      <th>Song Pinyin</th>\n",
       "      <th>Song Translation</th>\n",
       "      <th>Example Pinyin</th>\n",
       "      <th>Length</th>\n",
       "      <th>Character Phrase</th>\n",
       "      <th>Instagram Image Created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>1859</td>\n",
       "      <td>現實</td>\n",
       "      <td>现实</td>\n",
       "      <td>reality</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>xiàn shí</td>\n",
       "      <td>现实，就是 reality。</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tā bìxū zhèngshì shīyè zhè yī xiànshí</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4902</th>\n",
       "      <td>4905</td>\n",
       "      <td>屏障</td>\n",
       "      <td>屏障</td>\n",
       "      <td>protective screen</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>píng zhàng</td>\n",
       "      <td>屏障，就是防护屏。</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xīn shuǐbà shì jiānglái dǐyù hóngshuǐ de píngz...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2194</th>\n",
       "      <td>2196</td>\n",
       "      <td>單調</td>\n",
       "      <td>单调</td>\n",
       "      <td>monotonous, dull</td>\n",
       "      <td>5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>dān diào</td>\n",
       "      <td>单调，就是单调乏味。</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zhè liǎng gè jìjié zhījiān de rìzi shì fēichán...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id Traditional Simplified            English HSK  HSK 5（二）词语搭配    Img  \\\n",
       "1857  1859          現實         现实            reality   5           NaN  False   \n",
       "4902  4905          屏障         屏障  protective screen   6           NaN  False   \n",
       "2194  2196          單調         单调   monotonous, dull   5          20.0  False   \n",
       "\n",
       "        Txt      Pinyin     Explanation  ... Alternative Grammar Reference  \\\n",
       "1857  False    xiàn shí  现实，就是 reality。  ...         NaN               NaN   \n",
       "4902  False  píng zhàng       屏障，就是防护屏。  ...         NaN               NaN   \n",
       "2194  False    dān diào      单调，就是单调乏味。  ...         NaN               NaN   \n",
       "\n",
       "     Song Lyrics  Song YouTube Song Pinyin Song Translation  \\\n",
       "1857         NaN           NaN         NaN              NaN   \n",
       "4902         NaN           NaN         NaN              NaN   \n",
       "2194         NaN           NaN         NaN              NaN   \n",
       "\n",
       "                                         Example Pinyin  Length  \\\n",
       "1857              tā bìxū zhèngshì shīyè zhè yī xiànshí       2   \n",
       "4902  xīn shuǐbà shì jiānglái dǐyù hóngshuǐ de píngz...       2   \n",
       "2194  zhè liǎng gè jìjié zhījiān de rìzi shì fēichán...       2   \n",
       "\n",
       "     Character Phrase Instagram Image Created  \n",
       "1857              NaN                     NaN  \n",
       "4902              NaN                     NaN  \n",
       "2194              NaN                     NaN  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hsk.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18896, 5)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences = pd.read_csv(DATA_DIR/'sentences.tsv',sep='\\t')\n",
    "df_sentences.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Characters</th>\n",
       "      <th>Pinyin</th>\n",
       "      <th>Meaning</th>\n",
       "      <th>HSK average</th>\n",
       "      <th>Custom Ratio</th>\n",
       "      <th>sentence_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5810</th>\n",
       "      <td>有学生向路人散发传单.</td>\n",
       "      <td>yǒu xuésheng xiàng lùrén sànfā chuándān</td>\n",
       "      <td>There were students dishing out leaflets to pa...</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>3005810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10364</th>\n",
       "      <td>他当上了省长.</td>\n",
       "      <td>tā dāngshang le shěngzhǎng</td>\n",
       "      <td>He established himself as governor of the prov...</td>\n",
       "      <td>4.600</td>\n",
       "      <td>0.400</td>\n",
       "      <td>3010364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3970</th>\n",
       "      <td>他站在那里，双手插在口袋里。</td>\n",
       "      <td>tā zhàn zài nàli shuāngshǒu chā zài kǒudài lǐ</td>\n",
       "      <td>He stood there with his hands in his pockets.</td>\n",
       "      <td>3.667</td>\n",
       "      <td>0.444</td>\n",
       "      <td>3003970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Characters                                         Pinyin  \\\n",
       "5810      有学生向路人散发传单.        yǒu xuésheng xiàng lùrén sànfā chuándān   \n",
       "10364         他当上了省长.                     tā dāngshang le shěngzhǎng   \n",
       "3970   他站在那里，双手插在口袋里。  tā zhàn zài nàli shuāngshǒu chā zài kǒudài lǐ   \n",
       "\n",
       "                                                 Meaning  HSK average  \\\n",
       "5810   There were students dishing out leaflets to pa...        4.000   \n",
       "10364  He established himself as governor of the prov...        4.600   \n",
       "3970       He stood there with his hands in his pockets.        3.667   \n",
       "\n",
       "       Custom Ratio  sentence_index  \n",
       "5810          0.500         3005810  \n",
       "10364         0.400         3010364  \n",
       "3970          0.444         3003970  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences.columns = ['Characters', 'Pinyin', 'Meaning', 'HSK average',\n",
    "       'Custom Ratio']\n",
    "df_sentences['sentence_index'] = df_sentences.index + 3_000_000\n",
    "df_sentences.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping this row due to strange encoding behavior\n",
    "df_sentences.drop(15390,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18895"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences.to_sql('example_sentences',conn,if_exists='replace',index=df_sentences['sentence_index'])\n",
    "\n",
    "# Uncomment for debugging which rows aren't inserted\n",
    "\n",
    "#conn.close()\n",
    "#conn = sqlite3.connect('ccrs.db', isolation_level=None)\n",
    "#try:\n",
    "#    df_sentences.to_sql('example_sentences',conn,if_exists='replace',index=df_sentences['sentence_index'])\n",
    "#except Exception as e:\n",
    "#        print(f\"Error inserting row {df_links.loc[conn.total_changes]['sentence_index']} into database: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking Tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Dictionary to Example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords =  stopwordsiso.stopwords(['zh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cedict_index</th>\n",
       "      <th>sentence_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66512</th>\n",
       "      <td>2089256</td>\n",
       "      <td>3013498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15165</th>\n",
       "      <td>2018601</td>\n",
       "      <td>3007427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22135</th>\n",
       "      <td>2027785</td>\n",
       "      <td>3016730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cedict_index  sentence_index\n",
       "66512       2089256         3013498\n",
       "15165       2018601         3007427\n",
       "22135       2027785         3016730"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a linking table\n",
    "df_sentences['words'] = df_sentences['Characters'].apply(lambda x: [w for w in jieba.lcut(x) if w not in stopwords])\n",
    "\n",
    "df_sentences_exploded = df_sentences.explode('words').reset_index(drop=True)\n",
    "\n",
    "df_links = pd.merge(df_cedict, df_sentences_exploded, left_on='simplified', right_on='words')\n",
    "df_links = df_links[['cedict_index', 'sentence_index']].drop_duplicates().reset_index(drop=True)\n",
    "df_links.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87699"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# created a linking table\n",
    "df_links.to_sql('cedict_sentences',conn,if_exists='replace',index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link CEDICT WORDS TO Hanzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>traditional</th>\n",
       "      <th>simplified</th>\n",
       "      <th>pinyin</th>\n",
       "      <th>english</th>\n",
       "      <th>cedict_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105610</th>\n",
       "      <td>邊陲</td>\n",
       "      <td>边陲</td>\n",
       "      <td>bian1 chui2</td>\n",
       "      <td>border area</td>\n",
       "      <td>2105610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65869</th>\n",
       "      <td>漫天遍地</td>\n",
       "      <td>漫天遍地</td>\n",
       "      <td>man4 tian1 bian4 di4</td>\n",
       "      <td>lit. to fill the whole sky and cover the land;...</td>\n",
       "      <td>2065869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19978</th>\n",
       "      <td>含山縣</td>\n",
       "      <td>含山县</td>\n",
       "      <td>Han2 shan1 xian4</td>\n",
       "      <td>Hanshan county in Chaohu 巢湖[Chao2 hu2], Anhui</td>\n",
       "      <td>2019978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       traditional simplified                pinyin  \\\n",
       "105610          邊陲         边陲           bian1 chui2   \n",
       "65869         漫天遍地       漫天遍地  man4 tian1 bian4 di4   \n",
       "19978          含山縣        含山县      Han2 shan1 xian4   \n",
       "\n",
       "                                                  english  cedict_index  \n",
       "105610                                        border area       2105610  \n",
       "65869   lit. to fill the whole sky and cover the land;...       2065869  \n",
       "19978       Hanshan county in Chaohu 巢湖[Chao2 hu2], Anhui       2019978  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cedict.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_hanzi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb Cell 75\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#Y134sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_hanzi\u001b[39m.\u001b[39msample(\u001b[39m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_hanzi' is not defined"
     ]
    }
   ],
   "source": [
    "df_hanzi.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cjk(char):\n",
    "    char = ord(char)\n",
    "    cjk_ranges = [\n",
    "    (0x4E00,  0x62FF),\n",
    "    (0x6300,  0x77FF),\n",
    "    (0x7800,  0x8CFF),\n",
    "    (0x8D00,  0x9FCC),\n",
    "    (0x3400,  0x4DB5),\n",
    "    (0x20000, 0x215FF),\n",
    "    (0x21600, 0x230FF),\n",
    "    (0x23100, 0x245FF),\n",
    "    (0x24600, 0x260FF),\n",
    "    (0x26100, 0x275FF),\n",
    "    (0x27600, 0x290FF),\n",
    "    (0x29100, 0x2A6DF),\n",
    "    (0x2A700, 0x2B734),\n",
    "    (0x2B740, 0x2B81D),\n",
    "    (0x2B820, 0x2CEAF),\n",
    "    (0x2CEB0, 0x2EBEF),\n",
    "    (0x2F800, 0x2FA1F), ]\n",
    "    \n",
    "    \n",
    "    for bottom, top in cjk_ranges:\n",
    "        if char >= bottom and char <= top:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "df_cedict['simplified_chars'] = df_cedict['simplified'].apply(lambda x: ''.join([c for c in x if is_cjk(c)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract Chinese characters from the 'text' column and store them in a new column 'chinese'\n",
    "\n",
    "# split the Chinese characters in the 'chinese' column into a list and store the list in a new column 'chinese_list'\n",
    "df_cedict['simplified_chars'] = df_cedict['simplified_chars'].apply(list)\n",
    "\n",
    "# create a new DataFrame with unique Chinese characters and their indices\n",
    "#unique_chars = sorted(set(''.join(df_cedf_hanzidict['simplified_chars'].sum())))\n",
    "df = df_cedict[['cedict_index','simplified_chars']].explode('simplified_chars')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linking = df.merge(df_hanzi[['hanzi','hanzi_index']], left_on= 'simplified_chars', right_on='hanzi', how='left').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linking.dropna(how='any',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linking['hanzi_index'] = df_linking.hanzi_index.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linking = df_linking[['cedict_index','hanzi_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311635"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_linking.to_sql('hanzi_cedict',conn,if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1001712.0\n",
       "1         1000623.0\n",
       "2         1000426.0\n",
       "3         1000946.0\n",
       "4         1000426.0\n",
       "            ...    \n",
       "315885          NaN\n",
       "315886          NaN\n",
       "315887          NaN\n",
       "315888          NaN\n",
       "315889    1000073.0\n",
       "Name: hanzi_index, Length: 315890, dtype: float64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.hanzi_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_744625/3419132943.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_linking.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_linking.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hanzi to Radicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radical_number</th>\n",
       "      <th>radical</th>\n",
       "      <th>variants</th>\n",
       "      <th>pinyin</th>\n",
       "      <th>stroke_count</th>\n",
       "      <th>ucn</th>\n",
       "      <th>meaning</th>\n",
       "      <th>traditional</th>\n",
       "      <th>radical_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>44</td>\n",
       "      <td>尸</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shi1</td>\n",
       "      <td>3</td>\n",
       "      <td>U+5C38</td>\n",
       "      <td>[corpse, to impersonate the dead, to preside]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>91</td>\n",
       "      <td>片</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pian4</td>\n",
       "      <td>4</td>\n",
       "      <td>U+7247</td>\n",
       "      <td>[slice, splinter, strip, rad. 91]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>84</td>\n",
       "      <td>气</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qi4</td>\n",
       "      <td>4</td>\n",
       "      <td>U+6C14</td>\n",
       "      <td>[steam, vapor]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    radical_number radical variants pinyin  stroke_count     ucn  \\\n",
       "42              44       尸      NaN   shi1             3  U+5C38   \n",
       "89              91       片      NaN  pian4             4  U+7247   \n",
       "82              84       气      NaN    qi4             4  U+6C14   \n",
       "\n",
       "                                          meaning traditional  radical_index  \n",
       "42  [corpse, to impersonate the dead, to preside]         NaN         100042  \n",
       "89              [slice, splinter, strip, rad. 91]         NaN         100089  \n",
       "82                                 [steam, vapor]         NaN         100082  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EDA\n",
    "\n",
    "df_radicals.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of all radicals, varients and traditional \n",
    "unique_base_radicals =  pd.concat([df_radicals['radical'],df_radicals['traditional'],radical_variants_unique]).dropna().drop_duplicates(keep='first')\n",
    "\n",
    "#unique_radicals = pd.concat([df_radicals['radical'],df_radicals['variants'],df_radicals['rad']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decomp = pd.read_csv(DATA_DIR/'extracted'/'FlattenedDecompositionTable.csv',index_col=0,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8\n"
     ]
    }
   ],
   "source": [
    "import cchardet as chardet\n",
    "\n",
    "with open(DATA_DIR/'extracted'/'FlattenedDecompositionTable.csv','rb') as f :\n",
    "    result = chardet.detect(f.read())\n",
    "print(result['encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "# Checking to see if the components will eventually break down into characers\n",
    "main_component = df_decomp['Component']\n",
    "right_component =  df_decomp['RightComponent']\n",
    "left_component = df_decomp['LeftComponent']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "(main_component.str.len() > 1).sum()\n",
    "right_component[right_component.str.len() > 1 ].str.replace(' ','')\n",
    "left_component[left_component.str.len() > 1 ].str.replace(' ','')\n",
    "\n",
    "# Filter rows with multiple radicals\n",
    "\n",
    "breakdown_right_components = right_component[right_component.str.len() > 1]\n",
    "breakdown_left_components = left_component[left_component.str.len() > 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7450980392156863"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breakdown_left_components.unique().shape[0] / breakdown_left_components.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_base_radicals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb Cell 90\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#Y146sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m unique_base_radicals[unique_base_radicals \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m爫\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unique_base_radicals' is not defined"
     ]
    }
   ],
   "source": [
    "unique_base_radicals[unique_base_radicals == '爫']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Component             伶\n",
       "Strokes               7\n",
       "CompositionType       吅\n",
       "LeftComponent         亻\n",
       "LeftStrokes           2\n",
       "RightComponent        令\n",
       "RightStrokes          5\n",
       "Signature          OOII\n",
       "Notes                 /\n",
       "Section               人\n",
       "Name: 310, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_decomp.iloc[169]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_components =  pd.concat([main_component,right_component,left_component],axis=0).drop_duplicates(keep='first').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of the set of unique radicals that are in the set of unique components\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9549180327868853"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Percentage of the set of unique radicals that are in the set of unique components')\n",
    "unique_base_radicals.isin(unique_components).sum() / unique_base_radicals.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radicals that are not in the components_list\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22       匸\n",
       "33       夊\n",
       "2       乀 \n",
       "3       乚 \n",
       "41       尣\n",
       "118      ⺮\n",
       "162     阝 \n",
       "182      飠\n",
       "2        乁\n",
       "3        乛\n",
       "45      巜 \n",
       "dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Radicals that are not in the components_list')\n",
    "unique_base_radicals[~unique_base_radicals.isin(unique_components)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1           丁\n",
       "3           七\n",
       "7           万\n",
       "8           丈\n",
       "9           三\n",
       "         ... \n",
       "19760       𠚍\n",
       "19762     木缶木\n",
       "20521       𠤏\n",
       "20750    口口田一\n",
       "20835       歯\n",
       "Length: 10448, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_components[~unique_components.isin(unique_base_radicals)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Decomposition\n",
    "\n",
    "Layer 1, layer 2, Layer 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the tree structure ready, it would be good to \n",
    "set it up in a few layers of decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component</th>\n",
       "      <th>Strokes</th>\n",
       "      <th>CompositionType</th>\n",
       "      <th>LeftComponent</th>\n",
       "      <th>LeftStrokes</th>\n",
       "      <th>RightComponent</th>\n",
       "      <th>RightStrokes</th>\n",
       "      <th>Signature</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>丁</td>\n",
       "      <td>2</td>\n",
       "      <td>吕</td>\n",
       "      <td>一</td>\n",
       "      <td>1</td>\n",
       "      <td>亅</td>\n",
       "      <td>1</td>\n",
       "      <td>MN</td>\n",
       "      <td>/</td>\n",
       "      <td>一</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Component  Strokes CompositionType LeftComponent  LeftStrokes  \\\n",
       "1         丁        2               吕             一            1   \n",
       "\n",
       "  RightComponent  RightStrokes Signature Notes Section  \n",
       "1              亅             1        MN     /       一  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_decomp[df_decomp['Component'] == '丁']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ids_decomp = pd.read_csv(DATA_DIR/'extracted'/'idsDecomposition.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "queue = Queue()\n",
    "\n",
    "class HanziNode:\n",
    "    def __init__(self,val):\n",
    "        self.leftChild = None\n",
    "        self.rightChild = None\n",
    "        self.data = val\n",
    "\n",
    "\n",
    "    # Position Dict\n",
    "    # Order of traversal Inorder\n",
    "\n",
    "    # Print tree\n",
    "    def print_tree(self):\n",
    "        ret = []\n",
    "        ret.append(self.data)\n",
    "        if self.leftChild is not None:\n",
    "            queue.put(self.leftChild)\n",
    "        if self.rightChild is not None:\n",
    "            queue.put(self.rightChild)\n",
    "\n",
    "        #print (len(stack))\n",
    "        while queue.empty() is False:\n",
    "            ret = ret + queue.get().printTree() \n",
    "        return ret\n",
    "    \n",
    "    def preorder_traversal(self, root):\n",
    "        ret = []\n",
    "        if root:\n",
    "            ret.append(root.data)\n",
    "            ret = ret + self.preorderTraversal(root.leftChild)\n",
    "            ret = ret + self.preorderTraversal(root.rightChild)\n",
    "        return ret\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.leftChild is None and self.rightChild is None\n",
    "\n",
    "    def is_radical(self,radicals_col):\n",
    "        # Checks if the node is a radical\n",
    "        return radicals_col.str.contains(self.data).sum() != False\n",
    "    \n",
    "    def get_sub_components(self,df):\n",
    "        return df[df['Component'] == self.data]\n",
    "\n",
    "    def populate_tree(self, df_components):\n",
    "        components_df_row = self.get_sub_components(df_components)\n",
    "\n",
    "        if  components_df_row is not None or not self.is_radical(self.data):\n",
    "            leftComponent =  components_df_row['LeftComponent'].iloc[0]\n",
    "            rightComponent = components_df_row['RightComponent'].iloc[0]\n",
    "            #print(leftComponent)\n",
    "            #print(rightComponent)\n",
    "\n",
    "            if leftComponent is not None:\n",
    "                self.leftChild = HanziNode(leftComponent)\n",
    "            else:\n",
    "                self.leftChild = None\n",
    "                \n",
    "            if rightComponent is not None:\n",
    "                self.rightChild = HanziNode(rightComponent)\n",
    "            else:\n",
    "                self.rightChild = None\n",
    "                #self.rightChild.populate_tree(components_df_row) \n",
    "\n",
    "    def get_all_leaves(self):\n",
    "        if self.leftChild is None and self.rightChild is None:\n",
    "            return [self]\n",
    "        else:\n",
    "            leaves = []\n",
    "            if self.leftChild is not None:\n",
    "                leaves += self.leftChild.get_all_leaves()\n",
    "            if self.rightChild is not None:\n",
    "                leaves += self.rightChild.get_all_leaves()\n",
    "            return leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "hanziDecompTreeDict = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "radicalList = list(unique_base_radicals)\n",
    "for index,row in df_hanzi.iterrows():\n",
    "    hanzi = row['hanzi']\n",
    "    HanziRoot = HanziNode(hanzi)\n",
    "    hanziDecompTreeDict[index] = HanziRoot\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "noDataOnCharacterList  = []\n",
    "\n",
    "for index, node in hanziDecompTreeDict.items():\n",
    "\n",
    "    try:\n",
    "        node.populate_tree(df_decomp)\n",
    "    except:\n",
    "        noDataOnCharacterList.append(index)\n",
    "\n",
    "\n",
    "for i in noDataOnCharacterList:\n",
    "    hanziDecompTreeDict[i].rightChild = None\n",
    "    hanziDecompTreeDict[i].leftChild = None "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "noSecondLayerList = []\n",
    "\n",
    "for index, node in hanziDecompTreeDict.items():\n",
    "    try:\n",
    "        node.rightChild.populate_tree(df_decomp)\n",
    "        node.leftChild.populate_tree(df_decomp)\n",
    "    except:\n",
    "        noSecondLayerList.append(index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2\n",
    "\n",
    "for index, node in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "noThirdLayerList = []\n",
    "\n",
    "for index, node in hanziDecompTreeDict.items():     \n",
    "    try:\n",
    "        node.rightChild.leftChild.populate_tree(df_decomp)\n",
    "        node.leftChild.leftChild.populate_tree(df_decomp)\n",
    "        node.rightChild.rightChild.populate_tree(df_decomp)\n",
    "        node.rightChild.leftChild.populate_tree(df_decomp)\n",
    "    except:\n",
    "        noThirdLayerList.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input to Pandas Series object\n",
    "#def check_for_rare_characters(col):\n",
    "\n",
    "char_series = df_decomp['LeftComponent']\n",
    "char_series = char_series.dropna().str[0]\n",
    "# Convert each character to its hexadecimal representation\n",
    "char_series = char_series.apply( lambda x: ord(x))\n",
    "\n",
    "# Define Unicode sets\n",
    "common_set = set(range(0x4E00, 0xA000))  # \n",
    "extension_a_set = set(range(0x3400, 0x4E00))  #  Extension A\n",
    "extension_b_set = set(range(0x20000, 0x2A6E0))  #  Extension B\n",
    "extension_c_set = set(range(0x2A700, 0x2B740))  #  Extension C\n",
    "extension_d_set = set(range(0x2B740, 0x2B820))  #  Extension D\n",
    "extension_e_set = set(range(0x2B820, 0x2CEB0))  #  Extension E\n",
    "extension_f_set = set(range(0x2CEB0, 0x2EC00))  #  Extension F\n",
    "extension_g_set = set(range(0x30000, 0x31350))  #  Extension G\n",
    "extension_h_set = set(range(0x31350, 0x32400))  #  Extension H\n",
    "\n",
    "# Convert Unicode code points to strings and create pandas Series\n",
    "\n",
    "extension_a_chars = pd.Series([chr(cp) for cp in extension_a_set])\n",
    "extension_b_chars = pd.Series([chr(cp) for cp in extension_b_set])\n",
    "extension_c_chars = pd.Series([chr(cp) for cp in extension_c_set])\n",
    "extension_d_chars = pd.Series([chr(cp) for cp in extension_d_set])\n",
    "extension_e_chars = pd.Series([chr(cp) for cp in extension_e_set])\n",
    "extension_f_chars = pd.Series([chr(cp) for cp in extension_f_set])\n",
    "extension_g_chars = pd.Series([chr(cp) for cp in extension_g_set])\n",
    "extension_h_chars = pd.Series([chr(cp) for cp in extension_h_set])\n",
    "\n",
    "\n",
    "# Define vectorized functions for each Unicode set\n",
    "common_mask = char_series.isin(common_set)\n",
    "extension_a_mask = char_series.isin(extension_a_chars)\n",
    "extension_b_mask = char_series.isin(extension_b_chars)\n",
    "extension_c_mask = char_series.isin(extension_c_chars)\n",
    "extension_d_mask = char_series.isin(extension_d_chars)\n",
    "extension_e_mask = char_series.isin(extension_e_chars)\n",
    "extension_f_mask = char_series.isin(extension_f_chars)\n",
    "extension_g_mask = char_series.isin(extension_g_chars)\n",
    "extension_h_mask = char_series.isin(extension_h_chars)\n",
    "\n",
    "# Create new column that specifies which Unicode set each character belongs to\n",
    "# Create Series of character sets\n",
    "char_set_series = pd.Series('', index=char_series.index)\n",
    "char_set_series[common_mask] = ''\n",
    "char_set_series[extension_a_mask] = 'A'\n",
    "char_set_series[extension_b_mask] = 'B'\n",
    "char_set_series[extension_c_mask] = 'C'\n",
    "char_set_series[extension_d_mask] = 'D'\n",
    "char_set_series[extension_e_mask] = 'E'\n",
    "char_set_series[extension_f_mask] = 'F'\n",
    "char_set_series[extension_g_mask] = 'G'\n",
    "char_set_series[extension_h_mask] = 'H'\n",
    "\n",
    "#df['char_set'] = char_set_series\n",
    "\n",
    "#return char_series\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_tree(node):\n",
    "    \"\"\"\n",
    "    Recursively traverses the tree and returns a dictionary of nodes.\n",
    "    \"\"\"\n",
    "    if node is None:\n",
    "        return {}\n",
    "\n",
    "    left_descendant = traverse_tree(node.leftChild)\n",
    "    right_descendant = traverse_tree(node.rightChild)\n",
    "\n",
    "    node_dict = {\n",
    "        'data': node.data,\n",
    "        'left_child': left_descendant,\n",
    "        'right_child': right_descendant\n",
    "    }\n",
    "\n",
    "    return node_dict\n",
    "\n",
    "hanzi_idx = 1001\n",
    "# Example usage:\n",
    "tree_dict = traverse_tree(hanziDecompTreeDict[hanzi_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tree(tree, hanzi_idx):\n",
    "    result = []\n",
    "    counter = 0\n",
    "\n",
    "    def flatten_node(node, hanzi_idx):\n",
    "        nonlocal counter\n",
    "        counter += 1\n",
    "        \n",
    "        if counter > 1:\n",
    "            result.append((hanzi_idx, node['data'], counter - 1))\n",
    "\n",
    "        if node['left_child']:\n",
    "            flatten_node(node['left_child'], hanzi_idx )\n",
    "        if node['right_child']:\n",
    "            flatten_node(node['right_child'], hanzi_idx)\n",
    "\n",
    "    flatten_node(tree, hanzi_idx)\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_component_list = []\n",
    "\n",
    "for hanzi_idx, RootNode in hanziDecompTreeDict.items():\n",
    "    tree_dict = traverse_tree(RootNode)\n",
    "    flat_rows = flatten_tree(tree_dict,hanzi_idx)\n",
    "    flat_component_list = flat_component_list + flat_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hanzi_components = pd.DataFrame(flat_component_list, columns=['hanzi_index','component','position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out * inside of the dataset\n",
    "df_hanzi_components  = df_hanzi_components[df_hanzi_components['component'] != '*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49103"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hanzi_components.to_sql('hanzi_components',conn,if_exists='replace')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chengyu (Idioms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chengyu = pd.read_json(DATA_DIR/'chengyu_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     12610\n",
       "1       206\n",
       "2        67\n",
       "3        29\n",
       "4        22\n",
       "6         8\n",
       "5         5\n",
       "7         4\n",
       "8         3\n",
       "10        2\n",
       "9         2\n",
       "34        1\n",
       "20        1\n",
       "19        1\n",
       "18        1\n",
       "12        1\n",
       "Name: Frequency, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chengyu.Frequency.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Abbr</th>\n",
       "      <th>Chinese</th>\n",
       "      <th>ChineseExplanation</th>\n",
       "      <th>EnglishLiteral</th>\n",
       "      <th>EnglishFigurative</th>\n",
       "      <th>Pinyin</th>\n",
       "      <th>Example</th>\n",
       "      <th>ExampleTranslation</th>\n",
       "      <th>Origin</th>\n",
       "      <th>OriginTranslation</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9400</th>\n",
       "      <td>9316</td>\n",
       "      <td>tsqt</td>\n",
       "      <td>泰山其颓</td>\n",
       "      <td>旧时用于哀悼大家敬仰的人。</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>tài  shān  qí  tuí</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>《礼记·檀弓上》：“泰山其颓乎。梁木其坏乎。哲人其萎乎。”</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Abbr Chinese ChineseExplanation EnglishLiteral EnglishFigurative  \\\n",
       "9400  9316  tsqt    泰山其颓      旧时用于哀悼大家敬仰的人。            N/A               N/A   \n",
       "\n",
       "                  Pinyin Example ExampleTranslation  \\\n",
       "9400  tài  shān  qí  tuí                        N/A   \n",
       "\n",
       "                             Origin OriginTranslation  Frequency  \n",
       "9400  《礼记·檀弓上》：“泰山其颓乎。梁木其坏乎。哲人其萎乎。”               N/A          0  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chengyu.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def to_snake_case(s):\n",
    "    return re.sub(r'([A-Z])(?<!^)', r'_\\1', s).lower()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "chengyu_cols =  [to_snake_case(s) for s in df_chengyu.columns]\n",
    "chengyu_cols[0] = 'ID'\n",
    "\n",
    "df_chengyu.columns = chengyu_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linking table to characters\n",
    "\n",
    "# tokenize, remove stopwords\n",
    "df_chengyu['chinese_tokens'] = df_chengyu['chinese'].apply(lambda x: [w for w in jieba.lcut(x) if w not in stopwords])\n",
    "df_chengyu['chinese_explanation_tokens'] = df_chengyu['chinese_explanation'].apply(lambda x: [w for w in jieba.lcut(x) if w not in stopwords])\n",
    "\n",
    "df_chengyu[\"chinese_tokens\"] = df_chengyu[\"chinese_tokens\"].apply(set)\n",
    "df_chengyu[\"chinese_explanation_tokens\"] = df_chengyu[\"chinese_explanation_tokens\"].apply(set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cedict['simplified_tokens'] = df_cedict['simplified'].apply(lambda x: [w for w in jieba.lcut(x) if w not in stopwords])\n",
    "\n",
    "df_cedict[\"simplified_tokens\"] = df_cedict[\"simplified_tokens\"].apply(lambda x: set(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>abbr</th>\n",
       "      <th>chinese</th>\n",
       "      <th>chinese_explanation</th>\n",
       "      <th>english_literal</th>\n",
       "      <th>english_figurative</th>\n",
       "      <th>pinyin</th>\n",
       "      <th>example</th>\n",
       "      <th>example_translation</th>\n",
       "      <th>origin</th>\n",
       "      <th>origin_translation</th>\n",
       "      <th>frequency</th>\n",
       "      <th>chinese_tokens</th>\n",
       "      <th>chinese_explanation_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8910</td>\n",
       "      <td>swty</td>\n",
       "      <td>世外桃源</td>\n",
       "      <td>原指与现实社会隔绝、生活安乐的理想境界。后也指环境幽静生活安逸的地方。借指一种空想的脱离现实...</td>\n",
       "      <td>world outside peach place</td>\n",
       "      <td>originally meant an imaginary, ideal place sep...</td>\n",
       "      <td>shì  wài  táo  yuán</td>\n",
       "      <td>在这儿，在这～的仙境中，有了人世喧嚣的声音。（杨沫《青春之歌》第一部第三章）</td>\n",
       "      <td>N/A</td>\n",
       "      <td>晋·陶潜《桃花园记》描述的一个与世隔绝，没有遭到祸乱的美好地方。</td>\n",
       "      <td>N/A</td>\n",
       "      <td>34</td>\n",
       "      <td>{世外桃源}</td>\n",
       "      <td>{现实, 原指, 隔绝, 环境, 安乐, 社会, 借指, 斗争, 幽静, 理想境界, 空想,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  abbr chinese                                chinese_explanation  \\\n",
       "0  8910  swty    世外桃源  原指与现实社会隔绝、生活安乐的理想境界。后也指环境幽静生活安逸的地方。借指一种空想的脱离现实...   \n",
       "\n",
       "             english_literal  \\\n",
       "0  world outside peach place   \n",
       "\n",
       "                                  english_figurative               pinyin  \\\n",
       "0  originally meant an imaginary, ideal place sep...  shì  wài  táo  yuán   \n",
       "\n",
       "                                  example example_translation  \\\n",
       "0  在这儿，在这～的仙境中，有了人世喧嚣的声音。（杨沫《青春之歌》第一部第三章）                 N/A   \n",
       "\n",
       "                             origin origin_translation  frequency  \\\n",
       "0  晋·陶潜《桃花园记》描述的一个与世隔绝，没有遭到祸乱的美好地方。                N/A         34   \n",
       "\n",
       "  chinese_tokens                         chinese_explanation_tokens  \n",
       "0         {世外桃源}  {现实, 原指, 隔绝, 环境, 安乐, 社会, 借指, 斗争, 幽静, 理想境界, 空想,...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chengyu.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n",
      "'chinese_explanation_tokens'\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "def tokens_subset_of_characters(tokens, characters):\n",
    "    \"\"\"\n",
    "    Check if all tokens are a subset of the characters.\n",
    "    \"\"\"\n",
    "    token_set = set(tokens)\n",
    "    char_set = set(characters)\n",
    "    return int(token_set.issubset(char_set))\n",
    "\n",
    "#def fill_sparse_matrix(group, df_chengyu, result_matrix, i, n_cols):\n",
    "#    for j, chinese_explanation_tokens in enumerate(df_chengyu['chinese_explanation_tokens'].values):\n",
    "#        result_matrix[group.index, i * n_cols + j] = tokens_subset_of_characters(chinese_explanation_tokens, group['simplified'])\n",
    "\n",
    "def fill_sparse_matrix(group, df_chengyu, result_matrix, i, n_cols):\n",
    "    for j, chinese_explanation_tokens in enumerate(df_chengyu['chinese_explanation_tokens'].values):\n",
    "        index = group.index.values.astype(np.int32)\n",
    "        values = tokens_subset_of_characters(chinese_explanation_tokens, group.values)\n",
    "        result_matrix[index, i * n_cols + j] = values\n",
    "\n",
    "\n",
    "def applyParallel(df, df_chengyu, func, result_matrix, counter, max_memory):\n",
    "    # compute the number of rows and columns in the result matrix\n",
    "\n",
    "    def increment_counter(result, counter):\n",
    "        counter.get_lock().acquire()\n",
    "        counter.value += 1\n",
    "        counter.get_lock().release()\n",
    "\n",
    "    n_rows, n_cols = len(df), len(df_chengyu)\n",
    "\n",
    "    # create a pool of worker processes\n",
    "    pool = multiprocessing.Pool(processes=4)\n",
    "\n",
    "    # determine the chunk size and group the input dataframe into chunks\n",
    "    chunk_size = max(1, int(max_memory * 1e6 / n_cols / 4))\n",
    "    groups = [df.iloc[i:i+chunk_size] for i in range(0, n_rows, chunk_size)]\n",
    "\n",
    "    # fill in the entries of the result matrix using multiple processes\n",
    "    for i, group in enumerate(groups):\n",
    "        pool.apply_async(\n",
    "            fill_sparse_matrix,\n",
    "            args=(group, df_chengyu, result_matrix, i, n_cols),\n",
    "            callback=increment_counter,\n",
    "            error_callback=print\n",
    "        )\n",
    "\n",
    "    # wait for all processes to finish\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # convert the result matrix to a sparse matrix\n",
    "    result_sparse = result_matrix.tocsr()\n",
    "\n",
    "    return result_sparse\n",
    "\n",
    "max_memory = 32  # maximum memory usage in GB\n",
    "n_chunks = 24\n",
    "grouped = df_cedict.groupby(df_cedict.index // n_chunks)\n",
    "num_rows = len(df_cedict)\n",
    "num_cols = len(df_chengyu)\n",
    "\n",
    "# create a sparse matrix to store the results\n",
    "result_matrix = dok_matrix((num_rows, num_rows * num_cols), dtype=np.int32)\n",
    "\n",
    "# create a shared memory counter\n",
    "counter = multiprocessing.Value('i', 0)\n",
    "\n",
    "# apply the function in parallel and return a sparse matrix of the results\n",
    "result_sparse = applyParallel(df_cedict['simplified_tokens'], df_chengyu['chinese_explanation_tokens'], tokens_subset_of_characters, result_matrix, counter, max_memory)\n",
    "\n",
    "\n",
    "# convert the sparse matrix to a boolean array and assign it to a new column of the input dataframe\n",
    "#df_cedict['token_set'] = result_sparse.all(axis=1).toarray().ravel()\n",
    "#df_cedict['token_set'] = result_sparse.any(axis=1).toarray().ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1564413729 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_sparse[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000  # number of rows to process at a time\n",
    "n_rows = len(df_cedict)\n",
    "n_batches = (n_rows + chunk_size - 1) // chunk_size  # number of batches\n",
    "row_lengths = np.diff(result_sparse.indptr)\n",
    "\n",
    "matching_indexes = []  # initialize empty list to store matching indexes\n",
    "\n",
    "for i in range(n_batches):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min(start_idx + chunk_size, n_rows)\n",
    "    row_counts = result_sparse[start_idx:end_idx, :].sum(axis=1)\n",
    "    matching_rows, matching_cols = np.nonzero(row_counts == row_lengths[start_idx:end_idx])\n",
    "    matching_rows += start_idx  # adjust row indexes to match the original dataframe\n",
    "    matching_indexes.appendtoken_set((matching_rows, matching_cols))\n",
    "\n",
    "# flatten the list of matching indexes and create a boolean mask to assign True to matching rows and False to non-matching rows\n",
    "matching_rows = np.concatenate([x[0] for x in matching_indexes])\n",
    "matching_cols = np.concatenate([x[1] for x in matching_indexes])\n",
    "df_cedict['token_set'] = False\n",
    "df_cedict.iloc[matching_rows, df_cedict.columns.get_loc('token_set')] = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>traditional</th>\n",
       "      <th>simplified</th>\n",
       "      <th>pinyin</th>\n",
       "      <th>english</th>\n",
       "      <th>cedict_index</th>\n",
       "      <th>simplified_tokens</th>\n",
       "      <th>token_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019冠狀病毒病</td>\n",
       "      <td>2019冠状病毒病</td>\n",
       "      <td>er4 ling2 yi1 jiu3 guan1 zhuang4 bing4 du2 bing4</td>\n",
       "      <td>COVID-19, the coronavirus disease identified i...</td>\n",
       "      <td>2000000</td>\n",
       "      <td>{病, 冠状病毒, 2019}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21三體綜合症</td>\n",
       "      <td>21三体综合症</td>\n",
       "      <td>er4 shi2 yi1 san1 ti3 zong1 he2 zheng4</td>\n",
       "      <td>trisomy</td>\n",
       "      <td>2000001</td>\n",
       "      <td>{综合症, 21, 三体}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3C</td>\n",
       "      <td>3C</td>\n",
       "      <td>san1 C</td>\n",
       "      <td>abbr. for computers, communications, and consu...</td>\n",
       "      <td>2000002</td>\n",
       "      <td>{3C}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3P</td>\n",
       "      <td>3P</td>\n",
       "      <td>san1 P</td>\n",
       "      <td>(slang) threesome</td>\n",
       "      <td>2000003</td>\n",
       "      <td>{3P}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3Q</td>\n",
       "      <td>3Q</td>\n",
       "      <td>san1 Q</td>\n",
       "      <td>(Internet slang) thank you (loanword)</td>\n",
       "      <td>2000004</td>\n",
       "      <td>{3Q}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120678</th>\n",
       "      <td>𨭆</td>\n",
       "      <td>𬭶</td>\n",
       "      <td>hei1</td>\n",
       "      <td>hassium (chemistry)</td>\n",
       "      <td>2120678</td>\n",
       "      <td>{𬭶}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120679</th>\n",
       "      <td>𨭎</td>\n",
       "      <td>𬭳</td>\n",
       "      <td>xi3</td>\n",
       "      <td>seaborgium (chemistry)</td>\n",
       "      <td>2120679</td>\n",
       "      <td>{𬭳}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120680</th>\n",
       "      <td>𩧢</td>\n",
       "      <td>𱅒</td>\n",
       "      <td>cheng3</td>\n",
       "      <td>variant of 騁|骋[cheng3]</td>\n",
       "      <td>2120680</td>\n",
       "      <td>{𱅒}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120681</th>\n",
       "      <td>𰻞</td>\n",
       "      <td>𰻝</td>\n",
       "      <td>biang2</td>\n",
       "      <td>see 𰻞𰻞麵|𰻝𰻝面[biang2 biang2 mian4]</td>\n",
       "      <td>2120681</td>\n",
       "      <td>{𰻝}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120682</th>\n",
       "      <td>𰻞𰻞麵</td>\n",
       "      <td>𰻝𰻝面</td>\n",
       "      <td>biang2 biang2 mian4</td>\n",
       "      <td>broad, belt-shaped noodles, popular in Shaanxi</td>\n",
       "      <td>2120682</td>\n",
       "      <td>{𰻝, 面}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120683 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       traditional simplified  \\\n",
       "0        2019冠狀病毒病  2019冠状病毒病   \n",
       "1          21三體綜合症    21三体综合症   \n",
       "2               3C         3C   \n",
       "3               3P         3P   \n",
       "4               3Q         3Q   \n",
       "...            ...        ...   \n",
       "120678           𨭆          𬭶   \n",
       "120679           𨭎          𬭳   \n",
       "120680           𩧢          𱅒   \n",
       "120681           𰻞          𰻝   \n",
       "120682         𰻞𰻞麵        𰻝𰻝面   \n",
       "\n",
       "                                                  pinyin  \\\n",
       "0       er4 ling2 yi1 jiu3 guan1 zhuang4 bing4 du2 bing4   \n",
       "1                 er4 shi2 yi1 san1 ti3 zong1 he2 zheng4   \n",
       "2                                                 san1 C   \n",
       "3                                                 san1 P   \n",
       "4                                                 san1 Q   \n",
       "...                                                  ...   \n",
       "120678                                              hei1   \n",
       "120679                                               xi3   \n",
       "120680                                            cheng3   \n",
       "120681                                            biang2   \n",
       "120682                               biang2 biang2 mian4   \n",
       "\n",
       "                                                  english  cedict_index  \\\n",
       "0       COVID-19, the coronavirus disease identified i...       2000000   \n",
       "1                                                 trisomy       2000001   \n",
       "2       abbr. for computers, communications, and consu...       2000002   \n",
       "3                                       (slang) threesome       2000003   \n",
       "4                   (Internet slang) thank you (loanword)       2000004   \n",
       "...                                                   ...           ...   \n",
       "120678                                hassium (chemistry)       2120678   \n",
       "120679                             seaborgium (chemistry)       2120679   \n",
       "120680                             variant of 騁|骋[cheng3]       2120680   \n",
       "120681                   see 𰻞𰻞麵|𰻝𰻝面[biang2 biang2 mian4]       2120681   \n",
       "120682     broad, belt-shaped noodles, popular in Shaanxi       2120682   \n",
       "\n",
       "       simplified_tokens  token_set  \n",
       "0        {病, 冠状病毒, 2019}       True  \n",
       "1          {综合症, 21, 三体}       True  \n",
       "2                   {3C}       True  \n",
       "3                   {3P}       True  \n",
       "4                   {3Q}       True  \n",
       "...                  ...        ...  \n",
       "120678               {𬭶}       True  \n",
       "120679               {𬭳}       True  \n",
       "120680               {𱅒}       True  \n",
       "120681               {𰻝}       True  \n",
       "120682            {𰻝, 面}       True  \n",
       "\n",
       "[120683 rows x 7 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cedict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1000000 into shape (12963)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb Cell 124\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#Y245sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m row_counts \u001b[39m=\u001b[39m result_sparse[start_idx:end_idx, :]\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#Y245sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#df_cedict.iloc[start_idx:end_idx, df_cedict.columns.get_loc('simplified_tokens')] = (row_counts == row_lengths[start_idx:end_idx]).ravel()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#Y245sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df_cedict\u001b[39m.\u001b[39miloc[start_idx:end_idx, df_cedict\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mget_loc(\u001b[39m'\u001b[39m\u001b[39msimplified_tokens\u001b[39m\u001b[39m'\u001b[39m)] \u001b[39m=\u001b[39m (row_counts \u001b[39m==\u001b[39;49m row_lengths[start_idx:end_idx])\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, num_cols)\u001b[39m.\u001b[39mravel()\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1000000 into shape (12963)"
     ]
    }
   ],
   "source": [
    "chunk_size = 1000  # number of rows to process at a time\n",
    "n_rows = len(df_cedict)\n",
    "n_batches = (n_rows + chunk_size - 1) // chunk_size  # number of batches\n",
    "row_lengths = np.diff(result_sparse.indptr)\n",
    "\n",
    "for i in range(n_batches):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min(start_idx + chunk_size, n_rows)\n",
    "    row_counts = result_sparse[start_idx:end_idx, :].sum(axis=1)\n",
    "    #df_cedict.iloc[start_idx:end_idx, df_cedict.columns.get_loc('simplified_tokens')] = (row_counts == row_lengths[start_idx:end_idx]).ravel()\n",
    "    df_cedict.iloc[start_idx:end_idx, df_cedict.columns.get_loc('simplified_tokens')] = (row_counts == row_lengths[start_idx:end_idx]).reshape(-1, num_cols).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb Cell 124\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#Y243sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m row_counts \u001b[39m=\u001b[39m result_sparse\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#Y243sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m row_lengths \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdiff(result_sparse\u001b[39m.\u001b[39mindptr)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#Y243sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_cedict[\u001b[39m'\u001b[39m\u001b[39mtoken_set\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m row_counts \u001b[39m==\u001b[39m row_lengths\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3978\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3975\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3976\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3977\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> 3978\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4172\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4163\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4164\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4165\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4170\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4171\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4172\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   4174\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4175\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   4176\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   4177\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   4178\u001b[0m     ):\n\u001b[1;32m   4179\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4180\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4913\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4911\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m   4912\u001b[0m     com\u001b[39m.\u001b[39mrequire_length_match(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[0;32m-> 4913\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, allow_2d\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/construction.py:597\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure, allow_2d)\u001b[0m\n\u001b[1;32m    594\u001b[0m             subarr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(data, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m    595\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    596\u001b[0m         \u001b[39m# we will try to copy by-definition here\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m         subarr \u001b[39m=\u001b[39m _try_cast(data, dtype, copy, raise_cast_failure)\n\u001b[1;32m    599\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ABCExtensionArray):\n\u001b[1;32m    600\u001b[0m     \u001b[39m# it is already ensured above this is not a PandasArray\u001b[39;00m\n\u001b[1;32m    601\u001b[0m     subarr \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/construction.py:775\u001b[0m, in \u001b[0;36m_try_cast\u001b[0;34m(arr, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    773\u001b[0m arr \u001b[39m=\u001b[39m cast(np\u001b[39m.\u001b[39mndarray, arr)\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m arr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[39mreturn\u001b[39;00m sanitize_to_nanoseconds(arr, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    777\u001b[0m out \u001b[39m=\u001b[39m maybe_infer_to_datetimelike(arr)\n\u001b[1;32m    778\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m arr \u001b[39mand\u001b[39;00m copy:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1434\u001b[0m, in \u001b[0;36msanitize_to_nanoseconds\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     values \u001b[39m=\u001b[39m astype_overflowsafe(values, dtype\u001b[39m=\u001b[39mTD64NS_DTYPE)\n\u001b[1;32m   1433\u001b[0m \u001b[39melif\u001b[39;00m copy:\n\u001b[0;32m-> 1434\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39;49mcopy()\n\u001b[1;32m   1436\u001b[0m \u001b[39mreturn\u001b[39;00m values\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "row_counts = result_sparse.sum(axis=1)\n",
    "row_lengths = np.diff(result_sparse.indptr)\n",
    "df_cedict['token_set'] = row_counts == row_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120683, 6)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cedict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "RemoteError",
     "evalue": "\n---------------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/jentlejames/anaconda3/envs/ChineseAutomation/lib/python3.10/multiprocessing/managers.py\", line 209, in _handle_request\n    result = func(c, *args, **kwds)\n  File \"/home/jentlejames/anaconda3/envs/ChineseAutomation/lib/python3.10/multiprocessing/managers.py\", line 387, in create\n    obj = callable(*args, **kwds)\n  File \"/home/jentlejames/anaconda3/envs/ChineseAutomation/lib/python3.10/multiprocessing/managers.py\", line 1024, in Array\n    return array.array(typecode, sequence)\nTypeError: 'int' object is not iterable\n---------------------------------------------------------------------------",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb Cell 124\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#Y234sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m num_cols \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(df_chengyu)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#Y234sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# create shared memory array\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#Y234sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m#result_array = SynchronizedArray((num_rows, num_cols))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#Y234sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m result_array \u001b[39m=\u001b[39m manager\u001b[39m.\u001b[39;49mArray(\u001b[39m'\u001b[39;49m\u001b[39mi\u001b[39;49m\u001b[39m'\u001b[39;49m, num_rows \u001b[39m*\u001b[39;49m num_cols)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#Y234sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# create shared memory counter\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jentlejames/Projects/Data/ccrsApp/backend/nb/extract_data.ipynb#Y234sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m counter \u001b[39m=\u001b[39m multiprocessing\u001b[39m.\u001b[39mValue(\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ChineseAutomation/lib/python3.10/multiprocessing/managers.py:723\u001b[0m, in \u001b[0;36mBaseManager.register.<locals>.temp\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtemp\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m/\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m    722\u001b[0m     util\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mrequesting creation of a shared \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m object\u001b[39m\u001b[39m'\u001b[39m, typeid)\n\u001b[0;32m--> 723\u001b[0m     token, exp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create(typeid, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    724\u001b[0m     proxy \u001b[39m=\u001b[39m proxytype(\n\u001b[1;32m    725\u001b[0m         token, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_serializer, manager\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m    726\u001b[0m         authkey\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_authkey, exposed\u001b[39m=\u001b[39mexp\n\u001b[1;32m    727\u001b[0m         )\n\u001b[1;32m    728\u001b[0m     conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Client(token\u001b[39m.\u001b[39maddress, authkey\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_authkey)\n",
      "File \u001b[0;32m~/anaconda3/envs/ChineseAutomation/lib/python3.10/multiprocessing/managers.py:608\u001b[0m, in \u001b[0;36mBaseManager._create\u001b[0;34m(self, typeid, *args, **kwds)\u001b[0m\n\u001b[1;32m    606\u001b[0m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Client(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_address, authkey\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_authkey)\n\u001b[1;32m    607\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 608\u001b[0m     \u001b[39mid\u001b[39m, exposed \u001b[39m=\u001b[39m dispatch(conn, \u001b[39mNone\u001b[39;49;00m, \u001b[39m'\u001b[39;49m\u001b[39mcreate\u001b[39;49m\u001b[39m'\u001b[39;49m, (typeid,)\u001b[39m+\u001b[39;49margs, kwds)\n\u001b[1;32m    609\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    610\u001b[0m     conn\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/ChineseAutomation/lib/python3.10/multiprocessing/managers.py:93\u001b[0m, in \u001b[0;36mdispatch\u001b[0;34m(c, id, methodname, args, kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m kind \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m#RETURN\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m---> 93\u001b[0m \u001b[39mraise\u001b[39;00m convert_to_error(kind, result)\n",
      "\u001b[0;31mRemoteError\u001b[0m: \n---------------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/jentlejames/anaconda3/envs/ChineseAutomation/lib/python3.10/multiprocessing/managers.py\", line 209, in _handle_request\n    result = func(c, *args, **kwds)\n  File \"/home/jentlejames/anaconda3/envs/ChineseAutomation/lib/python3.10/multiprocessing/managers.py\", line 387, in create\n    obj = callable(*args, **kwds)\n  File \"/home/jentlejames/anaconda3/envs/ChineseAutomation/lib/python3.10/multiprocessing/managers.py\", line 1024, in Array\n    return array.array(typecode, sequence)\nTypeError: 'int' object is not iterable\n---------------------------------------------------------------------------"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "def tokens_subset_of_characters(tokens, characters, result_array, index):\n",
    "    \"\"\"\n",
    "    Check if all tokens are a subset of the characters.\n",
    "    \"\"\"\n",
    "    token_set = set(tokens)\n",
    "    char_set = set(characters)\n",
    "    result_array[index] = int(token_set.issubset(char_set))\n",
    "    \n",
    "\n",
    "class SynchronizedArray:\n",
    "    def __init__(self, shape):\n",
    "        self.arr = multiprocessing.Array('i', int(np.prod(shape)))\n",
    "        self.shape = shape\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.arr[idx]\n",
    "\n",
    "    def __setitem__(self, idx, value):\n",
    "        self.arr[idx] = value\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.shape[0]\n",
    "\n",
    "\n",
    "def increment_counter(result, counter):\n",
    "    counter.get_lock().acquire()\n",
    "    counter.value += 1\n",
    "    counter.get_lock().release()\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "\n",
    "max_memory = 32  # maximum memory usage in GB\n",
    "n_chunks = 12\n",
    "grouped = df_cedict.groupby(df_cedict.index // n_chunks)\n",
    "num_rows = len(df_cedict)\n",
    "num_cols = len(df_chengyu)\n",
    "\n",
    "# create shared memory array\n",
    "#result_array = SynchronizedArray((num_rows, num_cols))\n",
    "result_array = manager.Array('i', num_rows * num_cols)\n",
    "# create shared memory counter\n",
    "counter = multiprocessing.Value('i', 0)\n",
    "\n",
    "def applyParallel(dfGrouped, df_chengyu, func, result_array, counter):\n",
    "    with multiprocessing.Pool(12) as p:\n",
    "        for name, group in dfGrouped:\n",
    "            group_tokens = group['simplified'].values\n",
    "            for i, chinese_explanation_tokens in enumerate(df_chengyu['chinese_explanation_tokens'].values):\n",
    "                p.apply_async(\n",
    "                    func, \n",
    "                    args=(group_tokens, chinese_explanation_tokens, result_array, (counter.value, i)),\n",
    "                    callback=increment_counter,\n",
    "                    error_callback=print\n",
    "                )\n",
    "        p.close()\n",
    "        p.join()\n",
    "\n",
    "applyParallel(grouped, df_chengyu, tokens_subset_of_characters, result_array, counter)\n",
    "\n",
    "# convert shared memory array to numpy array\n",
    "result_np = np.frombuffer(result_array.arr.get_obj(), dtype=np.int32).reshape((num_rows, num_cols))\n",
    "\n",
    "df_cedict['token_set'] = result_np.all(axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weibo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChineseAutomation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
